---
layout: post
title:  "生成式模型的前世今生（三）考古-深度学习之前的生成式模型（1）"
date:   2021-09-02 20:20:54
categories: Notes
---
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
[上一篇] [源起-生成式模型的定义](https://yxgong0.github.io/notes/2021/07/07/gen2-1.html)
<br/>

<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</font><font size=2>如此大的篇幅必然存在错误，如果您发现了文中的错误，不论是文字、拼写、语法、概念或原理等，恳请您发送邮件至yxgong@std.uestc.edu.cn指正，不胜感激。我将定期发布勘误信息记录。</font>
<br/>

<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
近年来深度学习的发展十分迅速，但我并不打算直接从深度学习开始讲。深度学习出现之前，同样有着大量的经典且优秀的生成模型值得一提。所以，本章的内容就从高斯混合模型开始谈起。之所以拖了这么久，是因为我一直在考虑有没有什么方法能够不用公式就把这个模型讲清楚。最近就有些人在抱怨，学习机器学习理论的时候，原理其实并不复杂，但是公式看着太吃力了。很遗憾，我的结论是：没有。离开了公式，我没法把任何内容讲清楚，我能做的只是尽量把公式梳理清楚，把所需的基础知识尽量列举出来。</font>  

### 一、从高斯分布说起

<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;高斯分布，也叫正态分布，高中的数学课上就会讲这个东西。它最早是由法国数学家亚伯拉罕·棣莫弗求得的。对于一个随机变量$X$服从正态分布，记为$X\sim N(μ,σ^2)$。它的概率密度函数为：</font>  
<br/>
<font size=3>$f(x)=\frac{1}{\sqrt{2\pi}σ}exp(-\frac{(x-μ)^2}{2σ^2})$</font>  
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用代码可以画一个它的函数图像：</font>

![高斯分布](/images/2021gen3-1-1/image2.png)

<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里我们假设了期望值$μ=0$，标准差$σ=1$，这时称为标准正态分布。可以看到，它的概率密度函数曲线有一个峰；它不像骆驼有两个峰，不要弄混了。因此，这个曲线也被称为「钟形曲线」。这个钟是寺庙里老和尚敲的钟，不是家里墙上挂的钟。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;高斯分布想来不难理解，也没什么问题。那么它和高斯混合模型有什么关系，如何定义高斯混合模型呢？现在你需要想起关于生成式模型的概念，这里再贴一次：</font>

> 在概率统计理论中，生成模型是指能够随机生成观测数据的模型，尤其是在给定某些隐含参数的条件下。它给观测值和标注数据序列指定一个联合概率分布。 

<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以看到，生成模型需要根据一组数据，求出一个联合概率分布，这是生成模型的核心。那么求联合概率分布的方式是怎样的？随着机器学习这么多年的发展和深度学习的发展， 现在，已经可以做到一切都直接让网络自己去学的水平。至于具体怎么学，后续的章节中我们再去慢慢体会，现在让我们先踏实地从最原始的「石器时代」开始。</font>

### 二、高斯模型和最大似然估计

<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;显然，虽然机器学习方法想要实现学习数据分布的「自动化」，你也不能要求一开始就实现全自动，尤其是在那个理论尚不成熟，硬件条件也不佳的时代。想要求出一个联合概率分布，最简单直观的方式就是先人为地选定一个分布，然后让程序自动学习这个分布的参数。那么最常用的分布就是高斯分布了。现在，先来定义一个n维的随机向量$X=[X_1,X_2,...,X_n]^T$，其中$X_i$是一个随机变量。现在让它服从n维的高斯分布，其概率密度函数表示如下：</font>
<br/><br/>
<font size=3>$f(x_1,x_2,...,x_n)=\frac{1}{(2\pi)^\frac{n}{2}|\Sigma|^\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$</font>
<br/>
<font size=3>其中，$x$和$\mu$都成为了向量，$\Sigma$代表随机向量中包含的所有的随机变量的协方差矩阵。矩阵的行列式和逆的求法这里就不再讲了。以$n=2$为例，假设随机变量$X_1,X_2$相互独立，那么二维的标准正态分布概率密度函数的图像也可以用代码画出来：</font>

![二维高斯分布](/images/2021gen3-1-1/image3.png)

<font size=3>同样有一个峰。显然，一维高斯分布是n维高斯分布的一种情况，而n维高斯分布的应用范围显然更广，因为它能够建立更多随机变量之间的关系。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然我们人为地指定了一组数据服从高斯分布，我们构建的模型自然就应该称为高斯模型。指定分布后，机器学习需要学习的就是这个分布的参数，因此下一步便是构建一种学习的方案。首先，要有几组真实的数据，给定了n维高斯分布中n个随机变量的值，以及它们对应的函数值。这组数据便被称为训练数据，其中随机变量的值称为数据，函数值称为标签。接下来，机器学习中「学习」的基本思路是，设法构建一个函数，这个函数的值越大/越小，代表学习到的参数越接近真实的参数，称为「损失函数」。严格来说，所谓「真实的参数」，只是「最能让训练数据正确」的参数。然后机器学习就转化成了一个最优化问题。在理想情况下，我们希望求得损失函数的最大/最小值点，即全局最优解。一般的做法是对损失函数求导/求梯度，然后向梯度方向移动一定的步长。关于具体的理论，因为最优化理论是单独的一门学科，此处不再过多介绍，有兴趣可以学习这门课程。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里介绍一种简单的估计参数的方法：最大似然估计。借用一下知乎用户@忆臻&nbsp;在</font>[<font size=3>这篇文章</font>
](https://zhuanlan.zhihu.com/p/26614750)<font size=3>中用到的例子：</font>

>假如有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。我们想知道罐中白球和黑球的比例，但我们不能把罐中的球全部拿出来数。现在我们可以每次任意从已经摇匀的罐中拿一个球出来，记录球的颜色，然后把拿出来的球再放回罐中。这个过程可以重复，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少？

<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;显然，答案是70%。这个答案几乎不用动脑筋就能想到，但是如果要你说一下为什么，得出这个结论的原理是什么，恐怕你就会发现这不是表面上那么简单。我们来捋一下，首先，我们反复取100次球有70次是白球这种情况下，黑白球比例依然可以是任何值，即使只有1个白球也是如此。但是出现每一种黑白球比例的概率显然是不同的。只有1个白球的情况下，取100次有70次是白球的可能性非常低；如果黑白球一样多，取100次有70次白球的可能性就要高得多了；如果白球占70%，那么取100次有70次白球的可能性就非常高了。可以看到，我们思考的过程是：「如果黑白球比例是$\theta$，那么出现A情况的可能性会更高/低」。这个描述显然符合条件概率的定义，所以我们把它用概率的形式写一下：</font>
<br/>
<font size=3>$P(取100次有70次白球|黑白球比例是\theta)$</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果我们能写出上式的计算公式，那么每给定一种黑白球比例$\theta$，我们就可以计算出一个条件概率。把这个条件概率看作是关于$\theta$的函数，我们就能找到极值点，从而找到使条件概率最大的$\theta$。这种方法就叫做最大似然估计。似然这个词应该就是为了翻译likelihood而发明的，1999版的辞海中没有收录，很难解释它的准确意思。可能反过来用最大似然估计这种方法的原理，去解释似然这个词，显得还要简单一些。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在，语言的问题暂时不要管了，因为数学的问题还没解决完。我们捋清了思路，但是这个条件概率，同时也是一个关于条件$\theta$的函数，怎么计算呢？考虑到我们会算这个条件概率值之后，目的是求最大值，因此我们先把它从概率改写成函数的形式，称为似然函数：</font>
<br/>
<font size=3>$L(\theta|x) = P(x=X|\theta)$</font>
<br/>
<font size=3>这里$x$是观测值。在上面的例子中，我们取100次球后白球出现的次数就是观测值。这是一个变量，当我们进行观测这个行为之后，就会观测到$x=X$，即$X$是观测到的结果，是一个固定值。白球出现了70次就是一个观测到的结果。当然我们也可以进行多次观测，利用一组观测值进行计算，这样$x$就是一个向量。$\theta$是参数，在上述的例子中，罐子中白球所占的比例就是这个模型的参数，也是我们想要求的值。当我们知道了这个参数之后，以后再从罐子里取小球的时候，取出来是白球的概率我们就清楚了，我们就构建了一个能预测取出的球的颜色的模型。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在，让我们捡起前面提的高斯模型。为了简便起见，我们直接用一个一维高斯分布。多维的高斯分布纯粹就是增加运算量，原理都是一样的。我们把它的概率密度函数再写一次，并用$y$来表示函数值：</font>
<br/>
<font size=3>$y=\frac{1}{\sqrt{2\pi}σ}exp(-\frac{(x-μ)^2}{2σ^2})$</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在我们希望构建一个模型，每给定一组$x$，我们就能计算出$y$。举例而言，我们可以假设某大学男生的尺寸服从高斯分布。那么显然我们希望知道这个高斯分布的参数，这样就每在这个学校找到一个男生，他的尺寸是多少的概率我也能算出来，这样我卖东西的时候每个尺寸我备多少货我也清楚。对了，我说的是卖鞋，刚才说的就是脚的尺寸。那么显然，要进行计算我必须知道均值$\mu$和方差$\sigma$的值。为了得到这两个值，我去大学里做调研，这个过程就是数据的采集。我们询问了很多个人的尺寸，并算出了每个尺寸出现的频率来表示概率，这样我们就得到了很多组$x$和$y$的值，我现在要开始计算，在均值和协方差等于多少的时候，我得到的很多组$x$和$y$最有可能出现。根据上面的理论，我现在想知道似然函数怎么表达。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先，我们假设采集到的很多组$x$和$y$之间都是相互独立的，因为这样计算概率最简单。当它们之间相互独立的时候，这些值同时出现的概率，等于每组值单独出现的概率相乘，不需要考虑它们之间出现的概率关系。如果这些值之间不独立，我们同样可以计算似然函数，但是计算会变得极其复杂。相比起进行复杂的计算，我们在做调研、采集数据的时候确保数据之间相互独立更简单，所以不要给自己找麻烦。接下来，我们要计算的是，在某一组参数$\theta$下，得到这么多组$x$和$y$的概率是多少。j假设一共有$n$组数据，其中得到第一组的概率记为$y_1$，第二组的记为$y_2$，以此类推。记作：</font>
<br/>
<font size=3>$L(\theta|x)=\prod_{i=1}^ny_i=\prod_{i=1}^nf(x_i;\theta)=(2\pi\sigma^2)^{-\frac{n}{2}}exp(-\sum_{i=1}^{n}\frac{(x_i-\mu)^2}{2\sigma^2})$</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里你可能马上有一个疑问：嗯？概率密度函数的函数值，就是这个点的概率吗？我刚刚举的鞋码例子因为样本符合的是离散概率分布，所以你没感觉到有问题，你感觉每个数都能算出来。但是，概率密度函数是连续概率分布才有的东西，这很难用通俗的例子来解释清楚。如果你一定要问，我只能说，概率密度函数的函数值不是概率。但它是什么我也说不清楚，严格来说它是概率的变化率；就像我手里拿个实心的铁球，你问我铁球里某个点的密度是什么含义一样，你不能问我怎么计算这个点的质量，因为我只知道密度要乘上一定体积才行，不能乘上一个无穷小。连续分布的样本总是有这个问题，我在一条线上取到一个点的概率其实是0，因为点的长度是0，线的长度不是0，但这件事情就是发生了……总之，这个与生成式模型和机器学习的关系还不怎么大，感兴趣的话自己找地方学吧，或者有机会我单独写个博客来讨论这个问题。现在，你就姑且认为概率密度函数的值就是概率吧。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在，得到了似然函数，我们就要求出合适的参数值，让似然函数最大化。最大才表示出现我们采集到的数据的可能性最大，也符合最大似然估计的定义。找函数的最大值点当然就要求导，导数等于0的时候就是极值点，这在高中就学过了。至于求得的点是极大值还是极小值，是不是最大值，找到点后代入验证一下就好了，是比较简单的工作，不再赘述。现在只介绍求极值点的过程。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;显然，高斯分布的似然函数的求导是令人崩溃的，因为里面有个指数函数头上顶着那么一大坨复合函数。同时，因为每个值都是概率，在0到1之间，这一顿乘下来值也非常小，小数点后那么多位很不利于计算。硬求当然也不是求不出来，但是两边同时取个对数，把那一大坨摘下来显然能简单一些，反正对数函数是单调的，不影响极值点，还能放大一下函数值。两边同时取$ln$得到：</font>
<br/>
<font size=3>$lnL(\theta)=lnL(\mu,\sigma^2)=ln[(2\pi\sigma^2)^{-\frac{n}{2}}]exp(-\sum_{i=1}^n\frac{(x_i-\mu)^2}{2\sigma^2})$</font>
<br/>
<font size=3>$=ln[(2\pi\sigma^2)^{-\frac{n}{2}}]+ln[exp(-\sum_{i=1}^n\frac{(x_i-\mu)^2}{2\sigma^2})]$</font>
<br/>
<font size=3>$=ln[(2\pi\sigma^2)^{-\frac{n}{2}}]-\sum_{i=1}^n\frac{(x_i-\mu)^2}{2\sigma^2}$</font>
<br/>
<font size=3>$=-\frac{n}{2}(ln2\pi+ln\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2$</font>
<br/>
<font size=3>$=-\frac{n}{2}ln2\pi-\frac{n}{2}ln\sigma^2-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2$</font>
<br/>
<font size=3>这时候再求导就要舒服一些。对$\mu$求导的时候前两项都不用管了，对$\sigma^2$求导的时候第一项就不用管了，总还是比硬求好一些。结果如下：</font>
<br/>
<font size=3>$\frac{\partial lnL}{\partial \mu}=\frac{1}{\sigma^2}\sum_{i=1}^{n}(x_i-\mu)=0 \Leftrightarrow \sum_{i=1}^{n}(x_i-\mu)=0$</font>
<br/>
<font size=3>$\frac{\partial lnL}{\partial \sigma^2}=-\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}\sum_{i=1}^{n}(x_i-\mu)^2=0 \Leftrightarrow \sigma^2=\frac{1}{n}\sum_{i=1}^{n}(x_i-\mu)^2$</font>
<br/>
<font size=3>解得：</font>
<br/>
<font size=3>$\mu = \frac{1}{n}\sum_{i=1}^{n}x_i=\overline{x}$，即$x$的均值</font>
<br/>
<font size=3>$\sigma^2=\frac{1}{n}\sum_{i=1}^{n}(x_i-\mu)^2$</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在，把刚才调研得到的$n$组数据值代入，就能得到求得的两个参数值。把这两个值放到高斯分布的概率密度函数中，以后就可以直接利用这个函数来计算概率了，以后再也不用去费事调研了。</font>

### 三、高斯混合模型和EM算法

<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果你是跳过了上面的部分直接到这里来的，相信你肯定是有一定的基础，不需要去看高斯模型和最大似然估计。如果你刚看完上面的内容，我建议你稍事休息，以免精神出现问题。高斯混合模型这个名字其实也非常的直观，将多个高斯分布混合起来，就能用于构建一个高斯混合模型。混合的方式也非常简单粗暴：相加。当然组成大分布的每个小高斯分布可能会乘上一定的权重，概率密度表达式为：</font>
<br/>
<font size=3>$f(x)=\sum_{k=1}^{K}\alpha_kf_k(x)$</font>
<br/>
<font size=3>同样可以画一个它的图像：</font>
<br/>
![混合高斯分布](/images/2021gen3-1-1/image1.png)

<font size=3>这里$K$是子分布的个数，$\alpha_k$代表的是第$k$个子分布的权重，$f_k(x)$是第$k$个子分布的概率密度函数。图像中，我们假设有两个一维子分布，第一个子分布的均值和方差是-6和1，第二个子分布的均值和方差是8和2。值当然是随便选的，不同的均值会影响峰的位置，不同的方差会影响峰的高度。可以看到，混合高斯模型有更多的峰，能够更准确地描述一些场景。例如，我们想知道北方某城市所有蟑螂的体型大小分布，那么可以用一个高斯分布来表示。大部分蟑螂大概就是一颗花生那么大，估计构建的高斯分布的均值也就在这附近。可是，如果我想知道全国的蟑螂的体积分布，南方的蟑螂显然就不再符合这个高斯分布，因为它的均值可能在冰箱那么大附近（bushi）。这时，高斯分布就不再适合，因为不同地区的情况不一样。最好的方法就是利用混合高斯分布，南方的蟑螂体型大小概率密度会有一个峰，北方也会有一个峰，这个模型就能相对比较准确地描述国内的蟑螂了。现在有了模型和表达式，请你拿起纸和笔，用最大似然估计去算一下如何估计参数吧（纯洁的微笑）。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;聪明人根本不会去算。首先，式中$K$的值没法求导，它是一个加法计算式中的项数；这个还算能解决，因为我们可以直接指定一个值，第二个问题是，这么多个加法多项式相乘，光乘出来估计就要几百年，更不要说后面还要求导和求极值点了。事实上，这里包含了几个「隐变量」，$K$是一个，但我们可以通过根据经验强行指定来解决。但还有一个，就是其实你所采集的每组数据，都属于某一个子分布，比如一只蟑螂要么是南方的要么是北方的，但具体属于哪一类也是隐含的信息，你并不清楚。这个就不太好办了。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了让含有隐变量的模型能完成最大似然估计，1977年Dempster 等人总结提出了期望最大化算法（Expectation-Maximum算法，EM算法）。这是一种迭代算法，不再是原始最大似然估计那种能精确求出极值点的方法，只能接近极值点，而且可能只是局部最大值点。随着算法的发展，由于分布越来越复杂，求出全局最优解也会慢慢地变成一种奢望。优化算法后面也会越来越多，并适用于不同的场景，从现在起基本可以抛弃求出精确的最优解的想法了，几乎所有的算法都是设法接近最优解。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们先用文字解释一下EM算法的思想。既然隐变量是隐藏的，我们就把它拎出来：先计算出一组数据属于哪个子分布，然后对这个子分布参数进行最大似然估计就行了。那么问题来了，我们如何计算出一组数据属于哪个子分布呢？也很简单：代入到每个子分布中，看哪个值更接近。那么既然要代入，当然不能有未知数，所以子分布的参数从哪里来呢？很简单：用前面最大似然估计的结果啊。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你可能有一点点无语。这是个先有鸡还是先有蛋的问题啊！这构成死锁了啊！别着急，为了防止这几个线程间互相等待，我们引入「初始值」来实现破局。先随机初始化，或者根据经验强行指定每个子分布的参数，然后就能计算数据属于哪个子分布了。然后每次最大似然估计出来的结果更新子分布的参数，再回到第一步计算下一组数据属于哪个子分布。如此反复，直到满足一定的条件即可。当然，最开始指定的参数对最终的结果有影响，可能初始值的一点点差距，就导致最终结果巨大的差异。因此，也有一种做法是尝试随机取多次初始值，再将得到的多个结果取平均。取初始值的方式也已经有大量的研究，有很多文献可供参考。EM算法已被证明一定是能够收敛的，但是否能收敛到全局最优解处并不能保证。总之，还是上面解释过的：你只能尝试接近最优解，不要奢望有方法直接找到全局最优解。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在我们从数学的角度描述一下EM算法的流程。假设我们观测到的数据为$x=\{x_1,x_2,...,x_n\}$。隐变量，也就是一个数据属于哪个子分布，记为$z=\{z_1,z_2,...z_K\}$，其中$z_k$表示第$k$个子分布。混合高斯分布的参数记为$\theta$。于是条件概率分布$p(z|x,\theta)$表示在给定观测值和参数的条件下，这组观测值属于某个子分布的概率。</font>
<br/>
<font size=3>$1. 初始化模型参数\theta$</font>
<br/>
<font size=3>$2. E步：对每个数据x_i，计算其属于子分布z_k的概率p(z_k|x_i,\theta)，并得到联合分布的条件概率的数学期望E(z_k|x_i,\theta)。$</font>
<br/>
<font size=3>$p(z_k|x_i,\theta)=\frac{\alpha_kf_k(x_i)}{\sum_{j=1}^{K}\alpha_jf_j(x_i)}$</font>
<br/>
<font size=3>$E(z_k|x,\theta)=\sum_{i=1}^{n}p(z_k|x_i,\theta)x_i$</font>
<br/>
<font size=3>$3. M步：根据上一步得出的结果，对用极大似然估计对每个子分布的参数，包括均值、协方差矩阵和权重进行更新。$</font>
<br/>
<font size=3>$\mu_k=\frac{E(z_k|x,\theta)}{\sum_{i=1}^{n}p(z_k|x_i,\theta)}$</font>
<br/>
<font size=3>$\Sigma_k=\frac{\sum_{i=1}^{n}p(z_k|x_i,\theta)(x_i-\mu_k)(x_i-\mu_k)^T}{\sum_{i=1}^{n}p(z_k|x_i,\theta)}$</font>
<br/>
<font size=3>$\alpha_k=\frac{\sum_{i=1}^{n}p(z_k|x_i,\theta)}{n}$</font>
<br/>
<font size=3>$4. 判断更新后的参数变化量，如果满足给定的条件||\theta_{i+1}-\theta_i||<\epsilon，则算法结束，输出当前参数。否则，回到第2步。$</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以看到，E步的作用是，对于所有的数据，计算子分布$z_k$生成所有数据的数学期望。M步的作用是，利用最大似然估计法更新参数。这里均值不再是所有数据的均值，而是数据在各个子分布上的加权平均。方差也是类似的情况。同时，各个子分布的权重也需要更新，它是所有数据都属于这个子分布的概率的均值。</font>
<br/>
<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实上，只要数据分布是多个子分布的和，EM算法都可以发挥作用，它并不是专门为混合高斯模型提出的。不过这一章既然讨论的是混合高斯模型，EM算法也就不得不讲了。</font>
### 四、生成模型

<font size=3>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;讲了这么多，你可能还没意识到这些和生成模型有什么关系，所以我再稍微解释几句。事实上，高斯混合模型就是一个典型的生成模型。还是用上面蟑螂的例子，当你构建了高斯混合模型之后，你可以直接在脑海中想象到：一个体积很小的北方蟑螂，或者一个体积很大的南方蟑螂看上去都很自然，出现的概率也大，因为概率密度这两个地方有两个峰，代表较大的概率。而很大的北方蟑螂和很小的南方蟑螂出现的概率就要低得多。按照上一篇中我们给出的定义，这正是典型的「生成式模型思维」。如果构建判别式模型，那么需要先告诉我你这只蟑螂是南方的还是北方的，我再判断它的提及可能有多大。判别式模型中，隐变量需要直接给定，就没必要再搞什么EM算法了。说起来有那么多例子我为什么非要选个蟑螂，快恶心死了……</font>

[下一篇] 考古-深度学习之前的生成式模型（2）隐马尔可夫模型
